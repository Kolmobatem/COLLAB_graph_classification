{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Authors: Alexis Raja Brachet, Louis Chirol, Sophia Chirrane, Tanguy Olympie"
      ],
      "metadata": {
        "id": "fRmblp9V2MPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph classification on the COLLAB dataset"
      ],
      "metadata": {
        "id": "oZ_oizwy2azc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8621juKth7Rt"
      },
      "source": [
        "https://paperswithcode.com/sota/graph-classification-on-collab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages and import libraries"
      ],
      "metadata": {
        "id": "wMVU0M3b22jm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDl2NdFEIlhw"
      },
      "outputs": [],
      "source": [
        "!pip install dgl\n",
        "!pip install transformers\n",
        "\n",
        "import seaborn as sns\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQDFSg50Ilhr"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dataset"
      ],
      "metadata": {
        "id": "DOrc9qRs2FWN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unLNTG3cY8Ep"
      },
      "outputs": [],
      "source": [
        "# https://docs.dgl.ai/en/0.8.x/api/python/dgl.DGLGraph.html\n",
        "\n",
        "import dgl.data\n",
        "dataset = dgl.data.TUDataset('COLLAB')#, directory='~/datasets/TUDataset')\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('max_num_node : ', dataset.max_num_node)\n",
        "print('num_labels : ',dataset.num_labels)\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "print(f'Number of nodes: {data[0].num_nodes()}')\n",
        "print(f'Number of edges: {data[0].num_edges()}')\n",
        "print(f'Average node degree: {data[0].num_edges() / data[0].num_nodes():.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tinD5kxywgr0"
      },
      "source": [
        "# Getting to know the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GE3Hf3BJrMj",
        "outputId": "c5a04bc4-9f76-4644-fdec-d3a2a84a9178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n"
          ]
        }
      ],
      "source": [
        "l = len(dataset) #extract dataset length = 5000\n",
        "\n",
        "graph, label = dataset[0]\n",
        "graph1, label = dataset[1]\n",
        "print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suKj9KlOKnSF"
      },
      "outputs": [],
      "source": [
        "nx_g = dgl.to_networkx(graph)\n",
        "nx_g1 = dgl.to_networkx(graph1)\n",
        "\n",
        "### Some edges are represented multiple times\n",
        "l=[]\n",
        "for (a,b,w) in nx_g.edges(data=True):\n",
        "  if a==23:\n",
        "    l.append((a,b,w))\n",
        "l[:10]\n",
        "\n",
        "nx.draw_networkx(nx_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EDA"
      ],
      "metadata": {
        "id": "bplgF5vJrZx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore some basic properties of the graphs in the dataset"
      ],
      "metadata": {
        "id": "hu0njc9O3OFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [str(g[1].item()) for g in dataset]\n",
        "num_nodes = [g[0].num_nodes() for g in dataset]\n",
        "num_edges = [g[0].num_edges() for g in dataset]\n",
        "degrees = [g[0].in_degrees().float().mean().item() for g in dataset]\n",
        "\n",
        "diameters = []\n",
        "\n",
        "for g in tqdm(dataset):\n",
        "    graph = g[0]\n",
        "    # Compute the shortest path between all pairs of nodes\n",
        "    dist = nx.floyd_warshall_numpy(graph.to_networkx())\n",
        "    # Compute the diameter of the graph\n",
        "    diameters.append(dist.max())\n"
      ],
      "metadata": {
        "id": "abOUEywBrRjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A macro visualization of the data"
      ],
      "metadata": {
        "id": "4EcI45hr10qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the dataset: statistics over the number of nodes and edges, and the number of classes.\n",
        "print('Number of graphs:', len(dataset))\n",
        "print('Number of classes:', dataset.num_classes)\n",
        "print('Average number of nodes:', np.mean(num_nodes))\n",
        "print('Average number of edges:', np.mean(num_edges))\n",
        "print('Average node degree:', np.mean(degrees))\n",
        "print('Average diameter:', np.mean(diameters))\n",
        "\n",
        "# Same but in log scale with seaborn\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "fig, ax = plt.subplots(2, 3, figsize=(15, 6))\n",
        "sns.histplot(num_nodes, bins=20, ax=ax[0,0])\n",
        "ax[0,0].set_xlabel('Number of nodes')\n",
        "ax[0,0].set_ylabel('Number of graphs')\n",
        "ax[0,0].set_yscale('log')\n",
        "# Plot average as a vertical line\n",
        "ax[0,0].axvline(np.mean(num_nodes), color='red', linestyle='--')\n",
        "\n",
        "sns.histplot(num_edges, bins=20, ax=ax[0,1])\n",
        "ax[0,1].set_xlabel('Number of edges')\n",
        "ax[0,1].set_ylabel('Number of graphs')\n",
        "ax[0,1].set_yscale('log')\n",
        "# Plot average as a vertical line\n",
        "ax[0,1].axvline(np.mean(num_edges), color='red', linestyle='--')\n",
        "\n",
        "\n",
        "sns.histplot(degrees, bins=20, ax=ax[0,2])\n",
        "ax[0,2].set_xlabel('Average node degree')\n",
        "ax[0,2].set_ylabel('Number of graphs')\n",
        "ax[0,2].axvline(np.mean(degrees), color='red', linestyle='--')\n",
        "\n",
        "sns.histplot(diameters, bins=20, ax=ax[1,0])\n",
        "ax[1,0].set_xlabel('Diameter')\n",
        "ax[1,0].set_ylabel('Number of graphs')\n",
        "\n",
        "\n",
        "sns.histplot(labels, bins=dataset.num_classes, ax=ax[1,1])\n",
        "ax[1,1].set_xlabel('Class')\n",
        "ax[1,1].set_ylabel('Number of graphs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "daEzm3AErbx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A per class visualization"
      ],
      "metadata": {
        "id": "41rB184k1eSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'labels': labels, 'num_nodes': num_nodes, 'num_edges': num_edges,\n",
        "                   'degrees': degrees})\n",
        "df['diameter'] = diameters\n",
        "df.diameter = df.diameter.astype(str)\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(20, 4))\n",
        "sns.boxplot(x='labels', y='num_nodes', data=df, ax=ax[0],\n",
        "            palette='Set2')\n",
        "ax[0].set_xlabel('Class')\n",
        "ax[0].set_ylabel('Number of nodes')\n",
        "ax[0].set_yscale('log')\n",
        "sns.boxplot(x='labels', y='num_edges', data=df, ax=ax[1],\n",
        "            palette='Set2')\n",
        "ax[1].set_xlabel('Class')\n",
        "ax[1].set_ylabel('Number of edges')\n",
        "ax[1].set_yscale('log')\n",
        "sns.boxplot(x='labels', y='degrees', data=df, ax=ax[2],\n",
        "            palette='Set2')\n",
        "ax[2].set_xlabel('Class')\n",
        "ax[2].set_ylabel('Average node degree')\n",
        "ax[2].set_yscale('log')\n",
        "sns.histplot(data=df, x='diameter', hue='labels', multiple='stack',\n",
        "             bins=20, palette='Set2', discrete=True, shrink=0.8, ax=ax[3])\n",
        "ax[3].set_xlabel('Diameter')\n",
        "ax[3].set_ylabel('Count')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CWBQJtlmrc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wUIS89Jj7kM"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAwFg1kvumM-"
      },
      "source": [
        "Conversion to a weighted simple graph taken from\n",
        "\n",
        "https://stackoverflow.com/questions/15590812/networkx-convert-multigraph-into-simple-graph-with-weighted-edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G79A8zN3r0Bl"
      },
      "outputs": [],
      "source": [
        "# create weighted graph from multigraph\n",
        "def multi_graph_to_weighted(multigraph):\n",
        "  G = nx.Graph()\n",
        "  for u,v,data in multigraph.edges(data=True):\n",
        "      w = data['weight'] if 'weight' in data else 1.0\n",
        "      if G.has_edge(u,v):\n",
        "          G[u][v]['weight'] += w\n",
        "      else:\n",
        "          G.add_edge(u, v, weight=w)\n",
        "  return G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRYP0HU9kBWW"
      },
      "source": [
        "# Machine Learning techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJy4buikNuS"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlPWGcb4C7DZ",
        "outputId": "63e78908-0c42-4d9b-e945-6475b740a6a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:00<00:00, 121429.72it/s]\n"
          ]
        }
      ],
      "source": [
        "labels=[]\n",
        "for i in tqdm(range(len(dataset))): #len(dataset)\n",
        "  graph,label=dataset[i]\n",
        "  labels.append(int(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMVplbztkNI4",
        "outputId": "21cfba01-f257-4722-e7ed-baa3d89928db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [06:41<00:00, 12.45it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "clustering_coefficients=[]\n",
        "avg_nmbr_edges=[]\n",
        "densities=[]\n",
        "mean_average_degree_connectivities=[]\n",
        "std_average_degree_connectivities=[]\n",
        "diameters=[]\n",
        "max_number_connected_components=[]\n",
        "average_shortest_path_lengths=[]\n",
        "number_of_edges=[]\n",
        "number_of_nodes=[]\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(dataset))): #len(dataset)\n",
        "  graph,label=dataset[i]\n",
        "  nx_graph=dgl.to_networkx(graph)\n",
        "  weighted_graph=multi_graph_to_weighted(nx_graph)\n",
        "\n",
        "  clustering_coefficients.append(nx.average_clustering(weighted_graph))\n",
        "\n",
        "  avg_nmbr_edges.append(nx.number_of_edges(weighted_graph)/nx.number_of_nodes(weighted_graph))  \n",
        "\n",
        "  densities.append(nx.density(weighted_graph))\n",
        "\n",
        "  average_degree_connectivities=nx.average_degree_connectivity(weighted_graph)\n",
        "\n",
        "  mean_average_degree_connectivities.append(np.mean(np.array(list(average_degree_connectivities.values()))/weighted_graph.number_of_nodes()))\n",
        "  std_average_degree_connectivities.append(np.std(np.array(list(average_degree_connectivities.values()))/weighted_graph.number_of_nodes()))\n",
        "  diameters.append(nx.diameter(weighted_graph))\n",
        "\n",
        "  max_number_connected_components.append(len(max(nx.connected_components(weighted_graph), key=len)))\n",
        "\n",
        "  average_shortest_path_lengths.append(nx.average_shortest_path_length(weighted_graph))\n",
        "\n",
        "  number_of_edges.append(nx.number_of_edges(weighted_graph))\n",
        "\n",
        "  number_of_nodes.append(nx.number_of_nodes(weighted_graph))\n",
        "\n",
        "df=pd.DataFrame()\n",
        "df['clustering_coefficients']=clustering_coefficients\n",
        "df['avg_nmbr_edges']=avg_nmbr_edges\n",
        "df['densities']=densities\n",
        "df['mean_average_degree_connectivities']=mean_average_degree_connectivities ###To be tweaked to be usable\n",
        "df['std_average_degree_connectivities']=std_average_degree_connectivities ###To be tweaked to be usable\n",
        "df['diameters']=diameters\n",
        "df['max_number_connected_components']=max_number_connected_components\n",
        "df['average_shortest_path_lengths']=average_shortest_path_lengths\n",
        "df['number_of_edges']=number_of_edges\n",
        "df['number_of_nodes']=number_of_nodes\n",
        "\n",
        "df['labels']=labels\n",
        "df.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2ht5hYPoXpJ",
        "outputId": "d0afd60b-df2e-4142-87fc-584dcc1f66bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['clustering_coefficients', 'avg_nmbr_edges', 'densities', 'diameters',\n",
              "       'max_number_connected_components', 'average_shortest_path_lengths',\n",
              "       'number_of_edges', 'number_of_nodes', 'labels',\n",
              "       'mean_average_degree_connectivities',\n",
              "       'std_average_degree_connectivities'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBkCIRe7O__s"
      },
      "outputs": [],
      "source": [
        "features=['clustering_coefficients', 'avg_nmbr_edges', 'densities', 'diameters',\n",
        "       'max_number_connected_components', 'average_shortest_path_lengths',\n",
        "       'number_of_edges', 'number_of_nodes','mean_average_degree_connectivities',\n",
        "       'std_average_degree_connectivities']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=df[features]\n",
        "y=df['labels']\n",
        "\n",
        "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "\n",
        "X_train, X_validation, ytrain, y_validation = train_test_split(X_train_validation,y_train_validation,test_size=0.1,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LogisticClassifier=LogisticRegression()\n",
        "LogisticClassifier.fit(X_train_validation, y_train_validation)\n",
        "\n",
        "y_pred3 = LogisticClassifier.predict(X_test)\n",
        "print(\"Logistic Regression Classifier : \")\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred3))\n",
        "print('Classification report:', classification_report(y_test, y_pred3))"
      ],
      "metadata": {
        "id": "R_NnB4F-xomz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "2RJ1H0_URF5t",
        "outputId": "6f416d78-5b9a-485f-a1e4-6b4aa6e7dfc6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d6c1193141f1>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSVCclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSVCclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# models[\"SVC\"]=SVCclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "SVCclassifier = svm.SVC(kernel='linear', probability=True)\n",
        "SVCclassifier.fit(X_train_validation, y_train_validation)\n",
        "\n",
        "\n",
        "y_pred = SVCclassifier.predict(X_test)\n",
        "\n",
        "print(\"Support Vector Machine Classifier : \")\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Classification report:', classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyapogvYS3rN",
        "outputId": "ecb000ff-f25f-418c-dbdf-16570326c403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.764\n",
            "Classification report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.84      0.80       524\n",
            "           1       0.60      0.67      0.63       144\n",
            "           2       0.87      0.69      0.77       332\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.74      0.73      0.73      1000\n",
            "weighted avg       0.77      0.76      0.76      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RandForClassifier=RandomForestClassifier()\n",
        "RandForClassifier.fit(X_train_validation, y_train_validation)\n",
        "\n",
        "y_pred2 = RandForClassifier.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression Classifier : \")\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred2))\n",
        "print('Classification report:', classification_report(y_test, y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdK3mEcxvJWD",
        "outputId": "33883352-707e-44b4-e986-ff012044dba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importances of each feature : \n",
            "clustering_coefficients : 0.12350499486313828\n",
            "avg_nmbr_edges : 0.2037594017335099\n",
            "densities : 0.09763902829861067\n",
            "diameters : 0.002269931224985741\n",
            "max_number_connected_components : 0.05298391247248613\n",
            "average_shortest_path_lengths : 0.10188465635271458\n",
            "number_of_edges : 0.15640391636691753\n",
            "number_of_nodes : 0.05661566762329325\n",
            "mean_average_degree_connectivities : 0.10742706240834436\n",
            "std_average_degree_connectivities : 0.09751142865599963\n"
          ]
        }
      ],
      "source": [
        "print('Importance scores of each feature : ')\n",
        "\n",
        "for i in range(len(features)):\n",
        "  print(features[i] + ' : ' + str(RandForClassifier.feature_importances_[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ZAtAngkW7s"
      },
      "source": [
        "## Distance based methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvcTaviAkZhq"
      },
      "outputs": [],
      "source": [
        "graphs_0 , _ = dataset[:100]\n",
        "graphs_1 , _ = dataset[3000:3100] \n",
        "graphs_2 , _ = dataset[4800:4900] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ_pv3sYVmrU"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from networkx.algorithms.similarity import graph_edit_distance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.manifold import SpectralEmbedding\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# create example graphs and labels\n",
        "\n",
        "X = graphs_0 + graphs_1 + graphs_2\n",
        "X = [dgl.to_networkx(graph) for graph in X]\n",
        "X = [ multi_graph_to_weighted(nx_graph) for nx_graph in X]\n",
        "y = np.concatenate((np.zeros(len(graphs_0)), np.ones(len(graphs_1)), 2*np.ones(len(graphs_2))))\n",
        "\n",
        "# split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# define a function to comut distance between two graphs\n",
        "def ged_distance(G1, G2):\n",
        "    return graph_edit_distance(G1, G2)\n",
        "\n",
        "def rwd_distance(G1,G2): #OK\n",
        "    rw1 = SpectralEmbedding(n_components=1, affinity='precomputed').fit_transform(nx.to_numpy_array(G1))\n",
        "    rw2 = SpectralEmbedding(n_components=1, affinity='precomputed').fit_transform(nx.to_numpy_array(G2))\n",
        "    rwd = cosine_similarity(rw1, rw2)[0, 0]\n",
        "    return rwd\n",
        "\n",
        "def short_path_distance(G1,G2): #OK\n",
        "    return nx.average_shortest_path_length(G1) - nx.average_shortest_path_length(G2)\n",
        "\n",
        "def graphlet_distance(G1,G2):\n",
        "    X = [graph_from_networkx(G1), graph_from_networkx(G2)]\n",
        "    # Compute graphlet kernel\n",
        "    kernel = GraphletSampling()\n",
        "    K = kernel.fit_transform(X)\n",
        "    return K\n",
        "\n",
        "def wl_distance(G1,G2):\n",
        "    X = [graph_from_networkx(G1), graph_from_networkx(G2)]\n",
        "    # Compute graphlet kernel\n",
        "    kernel = ShortestPath(normalize=True)\n",
        "    K = kernel.fit_transform(np.array(X))\n",
        "    return K\n",
        "\n",
        "def diameter_distance(G1,G2): #OK\n",
        "    return abs(nx.diameter(G1) - nx.diameter(G2))\n",
        "\n",
        "class KNN:\n",
        "    \"\"\"K-Nearest Neighbors classifier\"\"\"\n",
        "    \n",
        "    def __init__(self, k=5, metric =ged_distance ):\n",
        "        self.k = k\n",
        "        self.metric = metric\n",
        "\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        \n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "        for k in tqdm(range(len(X))):\n",
        "            x = X[k]\n",
        "            # compute distances to all training samples\n",
        "            distances = [self._distance(x, x_train) for x_train in self.X_]\n",
        "            # find k nearest neighbors and their corresponding labels\n",
        "            k_nearest = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = self.y_[k_nearest]\n",
        "            \n",
        "            # predict the label of the sample based on the majority vote of the neighbors\n",
        "            classes, counts = np.unique(k_nearest_labels, return_counts=True)\n",
        "            max_count = np.max(counts)\n",
        "            candidates = classes[counts == max_count]\n",
        "            y_pred.append(np.random.choice(candidates))\n",
        "        \n",
        "        return y_pred\n",
        "    \n",
        "    def _distance(self, x1, x2):\n",
        "        return self.metric(x1,x2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "fS5Lok5oVpaj",
        "outputId": "8a8669e9-cf5b-4fed-c151-1f39c793b7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/60 [00:22<22:30, 22.89s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-318ebbb07d9c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiameter_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# compute accuracy score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e6770d39092c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# compute distances to all training samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;31m# find k nearest neighbors and their corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mk_nearest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e6770d39092c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# compute distances to all training samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;31m# find k nearest neighbors and their corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mk_nearest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e6770d39092c>\u001b[0m in \u001b[0;36m_distance\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-e6770d39092c>\u001b[0m in \u001b[0;36mdiameter_distance\u001b[0;34m(G1, G2)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdiameter_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#OK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/algorithms/distance_measures.py\u001b[0m in \u001b[0;36mdiameter\u001b[0;34m(G, e, usebounds, weight)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_extrema_bounding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"diameter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meccentricity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/algorithms/distance_measures.py\u001b[0m in \u001b[0;36meccentricity\u001b[0;34m(G, v, sp, weight)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbunch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/classes/backends.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0;34mf\"'{name}' not implemented by {plugin_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     )\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# Keep a handle to the original function to use when testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/algorithms/shortest_paths/generic.py\u001b[0m in \u001b[0;36mshortest_path_length\u001b[0;34m(G, source, target, weight, method)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Find paths to all nodes accessible from the source.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"unweighted\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_source_shortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dijkstra\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mpath_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_source_dijkstra_path_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/classes/backends.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0;34mf\"'{name}' not implemented by {plugin_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     )\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# Keep a handle to the original function to use when testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36msingle_source_shortest_path_length\u001b[0;34m(G, source, cutoff)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mnextlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_single_shortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36m_single_shortest_path_length\u001b[0;34m(adj, firstlevel, cutoff)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mnextlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthislevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train KNN classifier and make predictions on test set\n",
        "clf = KNN(k=3, metric = diameter_distance)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(y_pred, y_test)\n",
        "# compute accuracy score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa2eqNUNVqUf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "e207c252-5ae1-48ee-d107-5c1f18238a3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b0da74c63c52>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbalanced_accuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalanced_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "b_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "print(b_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh1N0US8VtIU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e7a018b8-b421-4904-bc0f-db6fbb40f3ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d83cf200f80e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming y_true and y_pred are your true and predicted labels, respectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define the class names (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Define the class names (if any)\n",
        "class_names = [\"Class 1\", \"Class 2\", \"Class 3\"]\n",
        "\n",
        "# Normalize the confusion matrix if desired\n",
        "normalize = True\n",
        "if normalize:\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Create the figure and subplot\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# Add the class names to the axis labels\n",
        "ax.set(xticks=np.arange(cm.shape[1]),\n",
        "       yticks=np.arange(cm.shape[0]),\n",
        "       xticklabels=class_names, yticklabels=class_names,\n",
        "       title=\"Confusion matrix\",\n",
        "       ylabel=\"True label\",\n",
        "       xlabel=\"Predicted label\")\n",
        "\n",
        "# Rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over the data and create the annotations\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, \"{:.2f}\".format(cm[i, j]),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0v0QWe86_P8"
      },
      "source": [
        "## GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the following architectures have been explored :\n",
        "\n",
        "- GraphSAGE\n",
        "- GAT\n",
        "\n",
        "We first derive the Node2Vec algorithm, then define the datalaoders to get the training, validation and test sets. Finally, both architectures are implemented and trained.\n"
      ],
      "metadata": {
        "id": "_XI6NzQo6moY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Node2Vec algorithm"
      ],
      "metadata": {
        "id": "gGp0iosKLsjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "#from node2vec import Node2Vec\n",
        "from scipy.sparse import *\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, auc\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "\n",
        "# Functions seen in previous lab \n",
        "\n",
        "def generate_samples(graph, train_set_ratio):\n",
        "    \"\"\"\n",
        "    Graph pre-processing step required to perform supervised link prediction\n",
        "    Create training and test sets\n",
        "    \"\"\"\n",
        "        \n",
        "    # --- Step 0: The graph must be connected ---\n",
        "    if nx.is_connected(graph) is not True:\n",
        "        raise ValueError(\"The graph contains more than one connected component!\")\n",
        "       \n",
        "    \n",
        "    # --- Step 1: Generate positive edge samples for testing set ---\n",
        "    residual_g = graph.copy()\n",
        "    test_pos_samples = []\n",
        "      \n",
        "    # Store the shuffled list of current edges of the graph\n",
        "    edges = list(residual_g.edges())\n",
        "    np.random.shuffle(edges)\n",
        "    \n",
        "    # Define number of positive test samples desired\n",
        "    test_set_size = int((1.0 - train_set_ratio) * graph.number_of_edges())\n",
        "    train_set_size = graph.number_of_edges() - test_set_size\n",
        "    num_of_pos_test_samples = 0\n",
        "    \n",
        "    # Remove random edges from the graph, leaving it connected\n",
        "    # Fill in the blanks\n",
        "    for edge in edges:\n",
        "        \n",
        "        # Remove the edge\n",
        "        residual_g.remove_edge(edge[0], edge[1])\n",
        "        \n",
        "        # Add the removed edge to the positive sample list if the network is still connected\n",
        "        if nx.is_connected(residual_g):\n",
        "            num_of_pos_test_samples += 1\n",
        "            test_pos_samples.append(edge)\n",
        "        # Otherwise, re-add the edge to the network\n",
        "        else: \n",
        "            residual_g.add_edge(edge[0], edge[1])\n",
        "        \n",
        "        # If we have collected enough number of edges for testing set, we can terminate the loop\n",
        "        if num_of_pos_test_samples == test_set_size:\n",
        "            break\n",
        "    \n",
        "    # Check if we have the desired number of positive samples for testing set \n",
        "    if num_of_pos_test_samples != test_set_size:\n",
        "        raise ValueError(\"Enough positive edge samples could not be found!\")\n",
        "\n",
        "        \n",
        "    # --- Step 2: Generate positive edge samples for training set ---\n",
        "    # The remaining edges are simply considered for positive samples of the training set\n",
        "    train_pos_samples = list(residual_g.edges())\n",
        "        \n",
        "        \n",
        "    # --- Step 3: Generate the negative samples for testing and training sets ---\n",
        "    # Fill in the blanks\n",
        "    non_edges = list(nx.non_edges(graph))\n",
        "    np.random.shuffle(non_edges)\n",
        "    \n",
        "    train_neg_samples = non_edges[:train_set_size] \n",
        "    test_neg_samples = non_edges[train_set_size:train_set_size + test_set_size]\n",
        "\n",
        "    \n",
        "    # --- Step 4: Combine sample lists and create corresponding labels ---\n",
        "    # For training set\n",
        "    train_samples = train_pos_samples + train_neg_samples\n",
        "    train_labels = [1 for _ in train_pos_samples] + [0 for _ in train_neg_samples]\n",
        "    # For testing set\n",
        "    test_samples = test_pos_samples + test_neg_samples\n",
        "    test_labels = [1 for _ in test_pos_samples] + [0 for _ in test_neg_samples]\n",
        "    \n",
        "    return residual_g, train_samples, train_labels, test_samples, test_labels\n",
        "\n",
        "\n",
        "def edge_prediction(node2embedding, train_samples, test_samples, train_labels, test_labels, feature_func=None, plot_roc=True):\n",
        "    \n",
        "    # --- Construct feature vectors for edges ---\n",
        "    if feature_func is None:\n",
        "        feature_func = lambda x,y: abs(x-y)\n",
        "    \n",
        "    # Fill in the blanks\n",
        "    train_features = [feature_func(node2embedding[edge[0]], node2embedding[edge[1]]) for edge in train_samples]\n",
        "    test_features = [feature_func(node2embedding[edge[0]], node2embedding[edge[1]]) for edge in test_samples]\n",
        "    \n",
        "    # --- Build the model and train it ---\n",
        "    # Fill in the blanks\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    train_preds = clf.predict_proba(train_features)[:, 1]\n",
        "    test_preds = clf.predict_proba(test_features)[:, 1]\n",
        "\n",
        "    # --- Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from predictions ---\n",
        "    # Fill in the blanks\n",
        "    fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    if not plot_roc:\n",
        "        return roc_auc\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.plot(fpr, tpr, color='darkred', label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='lightgray', linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    \n",
        "    return roc_auc\n",
        "\n",
        "import random\n",
        "def generate_random_walk(graph, root, L):\n",
        "    \"\"\"\n",
        "    :param graph: networkx graph\n",
        "    :param root: the node where the random walk starts\n",
        "    :param L: the length of the walk\n",
        "    :return walk: list of the nodes visited by the random walk\n",
        "    \"\"\"\n",
        "    walk = [root]\n",
        "    # Fill in the blanks\n",
        "\n",
        "    node = walk[-1]\n",
        "    for l in range(L):\n",
        "\n",
        "      voisins = list(graph.neighbors(node))\n",
        "\n",
        "      len_list = len(voisins)\n",
        "      indice = random.randint(0,len_list-1)\n",
        "\n",
        "      next = voisins[indice]\n",
        "\n",
        "      walk.append(next)\n",
        "      node = next\n",
        "\n",
        "    return walk\n",
        "def deep_walk(graph, N, L):\n",
        "    '''\n",
        "    :param graph: networkx graph\n",
        "    :param N: the number of walks for each node\n",
        "    :param L: the walk length\n",
        "    :return walks: the list of walks\n",
        "    '''\n",
        "    walks = []\n",
        "\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "    nodes = list(graph.nodes())\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "      random.shuffle(nodes)\n",
        "      \n",
        "      for node in nodes:\n",
        "\n",
        "        current_walk = generate_random_walk(graph, node, L)\n",
        "        walks.append(current_walk)\n",
        "        \n",
        "    return walks\n",
        "\n",
        "\n",
        "#deep_walk(graph, N=3, L=8)"
      ],
      "metadata": {
        "id": "18-3qxbZLSDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7eed2a-b3a9-4f69-a0c5-bdb4c43f24a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-44abedd988bd>:4: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the graph features for the weighted sampler"
      ],
      "metadata": {
        "id": "wZfF3jc_uPsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_feat = df.copy()\n",
        "feat_tens = torch.tensor(graph_feat.values)"
      ],
      "metadata": {
        "id": "sH35M8sEv1rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Laod the data and initialize the dataloaders :"
      ],
      "metadata": {
        "id": "N4wDjL3R7Zzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFMKjgMN_gCR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dgl\n",
        "import dgl.data\n",
        "from dgl.nn.pytorch.conv import GraphConv, SAGEConv, GATConv\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler,  WeightedRandomSampler\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "dataset = dgl.data.TUDataset('COLLAB')#, directory='~/datasets/TUDataset')\n",
        "print(f'Dataset: {dataset}:')\n",
        "print(\"length of dataset: \", len(dataset))\n",
        "print('max_num_node : ', dataset.max_num_node)\n",
        "print('num_labels : ' ,dataset.num_labels)\n",
        "\n",
        "data = dataset[0] \n",
        "print(\"First graph:\")\n",
        "print(f'Number of nodes: {data[0].num_nodes()}')\n",
        "print(f'Number of edges: {data[0].num_edges()}')\n",
        "print(f'Average node degree: {data[0].num_edges() / data[0].num_nodes():.2f}')\n",
        "\n",
        "# Add a constant node feature.\n",
        "data[0].ndata['x'] = torch.ones(data[0].num_nodes(), 1)\n",
        "\n",
        "# Split dataset into training, val and test, shuffling the data first\n",
        "\n",
        "train_validation_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=.1)\n",
        "len(train_validation_idx)\n",
        "train_idx, validation_idx = train_test_split(list(range(len(train_validation_idx))), test_size=.1)\n",
        "\n",
        "train_validation_dataset=Subset(dataset, train_validation_idx)\n",
        "test_dataset=Subset(dataset,test_idx)\n",
        "\n",
        "train_dataset=Subset(train_validation_dataset,train_idx)\n",
        "validation_dataset=Subset(train_validation_dataset,validation_idx)\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "num_test = len(test_dataset)\n",
        "num_val = len(validation_dataset)\n",
        "\n",
        "\n",
        "#### Define a weighted sampler to address imbalanced data issue ###\n",
        "\n",
        "class_sample_count = np.array(\n",
        "[len(np.where(df.iloc[train_dataset.indices][\"labels\"] == t)[0]) for t in np.unique(df.iloc[train_dataset.indices][\"labels\"])])\n",
        "\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[t] for t in df.iloc[train_dataset.indices][\"labels\"]])\n",
        "samples_weight = torch.from_numpy(samples_weight)  \n",
        "train_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight),replacement=True)\n",
        "\n",
        "\n",
        "class_sample_count = np.array(\n",
        "[len(np.where(df.iloc[validation_dataset.indices][\"labels\"] == t)[0]) for t in np.unique(df.iloc[validation_dataset.indices][\"labels\"])])\n",
        "\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[t] for t in df.iloc[validation_dataset.indices][\"labels\"]])\n",
        "samples_weight = torch.from_numpy(samples_weight) \n",
        "val_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight),replacement=True)\n",
        "\n",
        "class_sample_count = np.array([len(np.where(df.iloc[test_dataset.indices][\"labels\"] == t)[0]) for t in df.iloc[test_dataset.indices][\"labels\"]])\n",
        "\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[t] for t in df.iloc[test_dataset.indices][\"labels\"]])\n",
        "samples_weight = torch.from_numpy(samples_weight) \n",
        "test_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight),replacement=True)\n",
        "\n",
        "\n",
        "##### Node2Vec input ######\n",
        "\n",
        "num_of_walks= 2\n",
        "walk_length= 5\n",
        "w2v_size = 32\n",
        "embedding_size = w2v_size\n",
        "window_size = 2\n",
        "output_filename=\"graph.embedding\"\n",
        "\n",
        "def Node2Vector(graph,num_of_walks,walk_length,embedding_size,window_size,output_filename):\n",
        "    # Perform random walks - call function\n",
        "    walks = deep_walk(dgl.to_networkx(graph), N=num_of_walks, L=walk_length)#\n",
        "    # Learn representations of nodes - use Word2Vec\n",
        "    model2V = Word2Vec(sentences=walks, vector_size=embedding_size, window=window_size, min_count=1, workers=4,hs=0) #\n",
        "    # Save the embedding vectors\n",
        "    model2V.wv.save_word2vec_format(output_filename)\n",
        "\n",
        "    node2embedding=model2V.wv\n",
        "    nodelist = list(dgl.to_networkx(graph))\n",
        "    word2vec_embeddings = np.asarray([node2embedding[node] for node in nodelist])\n",
        "    w2v_emb_tensor = torch.from_numpy(word2vec_embeddings)\n",
        "    return w2v_emb_tensor\n",
        "\n",
        "def collate(samples):\n",
        "    # The input `samples` is a list of pairs\n",
        "    #  (graph, label).\n",
        "    graphs, labels = map(list, zip(*samples))\n",
        "    # Add a constant node feature.\n",
        "    i=0\n",
        "    for g in graphs:\n",
        "        g.ndata['x'] = Node2Vector(g,num_of_walks,walk_length,embedding_size,window_size,output_filename) #torch.ones(g.num_nodes(), 1)\n",
        "        i+=1\n",
        "    batched_graph = dgl.batch(graphs)\n",
        "    return batched_graph, torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GraphSAGE architeture"
      ],
      "metadata": {
        "id": "mwYsO2uL7gHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTNl3y14-5mB"
      },
      "outputs": [],
      "source": [
        "# Build a classification model with SAGE convolutions layers\n",
        "class SAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes, aggregator_type='mean'):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type)\n",
        "        self.conv2 = SAGEConv(h_feats, h_feats, aggregator_type)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.mlp1 = nn.Linear(h_feats + in_feats, (h_feats + in_feats)//2)\n",
        "        self.mlp2 = nn.Linear((h_feats + in_feats)//2, (h_feats + in_feats)//4)\n",
        "        self.mlp3 = nn.Linear((h_feats + in_feats)//4, num_classes)\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(h_feats)\n",
        "        self.bn2 = nn.BatchNorm1d(h_feats)\n",
        "        self.bn3 = nn.BatchNorm1d(h_feats + in_feats)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        m = nn.Softmax(dim=1)\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.bn1(h)\n",
        "        h = self.conv2(g,h)\n",
        "        h = F.relu(h)\n",
        "        h = self.bn2(h)\n",
        "        h  = torch.cat((feats,h),dim=1)\n",
        "        h = self.bn3(h)\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            # Calculate graph representation by average readout.\n",
        "            hg = dgl.mean_nodes(g, 'h')\n",
        "        hg = self.mlp1(hg)\n",
        "        hg = F.relu(hg)\n",
        "        hg = self.mlp2(hg)\n",
        "        hg = F.relu(hg)\n",
        "        hg = self.mlp3(hg)\n",
        "        hg = m(hg)\n",
        "        return hg\n",
        "\n",
        "# Hyperparameters\n",
        "in_feats = embedding_size \n",
        "h_feats = 128\n",
        "batch_size = 64\n",
        "\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = GraphDataLoader(train_dataset, batch_size=batch_size, collate_fn=collate, sampler=train_sampler)\n",
        "validation_loader = GraphDataLoader(validation_dataset, batch_size=batch_size,collate_fn=collate, sampler=val_sampler)\n",
        "test_loader = GraphDataLoader(test_dataset, batch_size=batch_size, collate_fn=collate, sampler=test_sampler)\n",
        "\n",
        "\n",
        "import transformers\n",
        "\n",
        "# Create the model\n",
        "lr = 1e-3\n",
        "\n",
        "model = SAGE(in_feats, h_feats, dataset.num_labels).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "num_epochs = 100\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "\n",
        "#define a learning scheduler\n",
        "lr_scheduler = transformers.get_scheduler(\n",
        "    name=\"linear\", optimizer=opt, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    for batched_graph, labels in tqdm(train_loader):\n",
        "        batched_graph = batched_graph.to(device)\n",
        "        feats = batched_graph.ndata['x'].float().to(device)\n",
        "\n",
        "        labels = labels.long().to(device)\n",
        "        logits = model(batched_graph, feats)\n",
        "\n",
        "        loss = model.loss_fn(logits, labels)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        train_losses.append(loss.detach().item())\n",
        "        pred = logits.argmax(1)\n",
        "        train_correct += (pred == labels).sum().item()\n",
        "    epoch_train_acc = train_correct/num_train\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_correct = 0\n",
        "    for batched_graph, labels in validation_loader:\n",
        "        batched_graph = batched_graph.to(device)\n",
        "        feats = batched_graph.ndata['x'].float().to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(batched_graph, feats)\n",
        "            loss = model.loss_fn(logits, labels)\n",
        "            val_losses.append(loss.detach().item())\n",
        "        pred = logits.argmax(1)\n",
        "        val_correct += (pred == labels).sum().item()\n",
        "    epoch_val_acc = val_correct/num_val\n",
        "\n",
        "    train_loss_history.append(np.mean(train_losses))\n",
        "    train_acc_history.append(epoch_train_acc)\n",
        "    val_loss_history.append(np.mean(val_losses))\n",
        "    val_acc_history.append(epoch_val_acc)\n",
        "\n",
        "    print(f'Epoch {epoch} | Train Loss {np.mean(train_losses):.4f} | Train accuracy {epoch_train_acc}')\n",
        "    print(f'Epoch {epoch} | Val Loss {np.mean(val_losses):.4f} | Val accuracy {epoch_val_acc}')\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "for batched_graph, labels in tqdm(test_loader):\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    feats = batched_graph.ndata['x'].float().to(device)\n",
        "    labels = labels.long().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(batched_graph, feats)\n",
        "        pred = logits.argmax(1)\n",
        "        test_correct += (pred == labels).sum().item()\n",
        "\n",
        "test_acc = test_correct / num_test\n",
        "print(f'Test accuracy {test_acc:.4f}')\n",
        "\n",
        "# Plot the losses\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss_history, label=\"Train loss\")\n",
        "plt.plot(val_loss_history, label=\"Val loss\")\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_acc_history, label=\"Train acc\")\n",
        "plt.plot(val_acc_history, label=\"Val acc\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GATConv"
      ],
      "metadata": {
        "id": "aLGhmvXBh0tT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1BHQ-5vvvZw"
      },
      "outputs": [],
      "source": [
        "# Build a classification model with GAT convolutions layers\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes, heads = 4):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(in_feats, h_feats, num_heads = heads, residual = True)\n",
        "        self.conv2 = GATConv(h_feats, h_feats,num_heads = heads, residual = True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.mlp1 = nn.Linear(h_feats, (h_feats)//2)\n",
        "        self.mlp2 = nn.Linear((h_feats)//2, (h_feats)//4)\n",
        "        self.mlp3 = nn.Linear((h_feats)//4, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        h = F.relu(h)\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            # Calculate graph representation by average readout.\n",
        "            hg = dgl.mean_nodes(g, 'h')\n",
        "        hg = self.mlp1(hg)\n",
        "        hg = F.relu(hg)\n",
        "        hg = self.mlp2(hg)\n",
        "        hg = F.relu(hg)\n",
        "        hg = self.mlp3(hg)\n",
        "        return hg\n",
        "\n",
        "# Hyperparameters\n",
        "in_feats = embedding_size \n",
        "h_feats = 128\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# DataLoader\n",
        "\n",
        "train_loader = GraphDataLoader(train_dataset, batch_size=batch_size, collate_fn=collate, sampler=train_sampler)\n",
        "print(train_loader)\n",
        "validation_loader = GraphDataLoader(validation_dataset, batch_size=batch_size,collate_fn=collate, sampler=val_sampler)\n",
        "test_loader = GraphDataLoader(test_dataset, batch_size=batch_size, collate_fn=collate, sampler=test_sampler)\n",
        "\n",
        "\n",
        "import transformers\n",
        "\n",
        "# Create the model\n",
        "lr = 1e-3\n",
        "\n",
        "model = GAT(in_feats, h_feats, dataset.num_labels).to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "#optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "\n",
        "lr_scheduler = transformers.get_scheduler(\n",
        "    name=\"linear\", optimizer=opt, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    for batched_graph, labels in tqdm(train_loader):\n",
        "        batched_graph = batched_graph.to(device)\n",
        "        feats = batched_graph.ndata['x'].float().to(device)\n",
        "        labels = labels.long().to(device)\n",
        "        logits = model(batched_graph, feats)\n",
        "        logits = torch.mean(logits,dim=1)\n",
        "        logits = torch.mean(logits,dim=1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        lr_scheduler.step()\n",
        "        train_losses.append(loss.detach().item())\n",
        "        pred = logits.argmax(1)\n",
        "        train_correct += (pred == labels).sum().item()\n",
        "    epoch_train_acc = train_correct/num_train\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_correct = 0\n",
        "    for batched_graph, labels in validation_loader:\n",
        "        batched_graph = batched_graph.to(device)\n",
        "        feats = batched_graph.ndata['x'].float().to(device)\n",
        "        labels = labels.long().to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(batched_graph, feats)\n",
        "            logits = torch.mean(logits,dim=1)\n",
        "            logits = torch.mean(logits,dim=1)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            val_losses.append(loss.detach().item())\n",
        "        pred = logits.argmax(1)\n",
        "        val_correct += (pred == labels).sum().item()\n",
        "    epoch_val_acc = val_correct/num_val\n",
        "\n",
        "    train_loss_history.append(np.mean(train_losses))\n",
        "    train_acc_history.append(epoch_train_acc)\n",
        "    val_loss_history.append(np.mean(val_losses))\n",
        "    val_acc_history.append(epoch_val_acc)\n",
        "\n",
        "    print(f'Epoch {epoch} | Train Loss {np.mean(train_losses):.4f} | Train accuracy {epoch_train_acc}')\n",
        "    print(f'Epoch {epoch} | Val Loss {np.mean(val_losses):.4f} | Val accuracy {epoch_val_acc}')\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "for batched_graph, labels in tqdm(test_loader):\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    feats = batched_graph.ndata['x'].float().to(device)\n",
        "    labels = labels.long().to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(batched_graph, feats)\n",
        "        logits = torch.mean(logits,dim=1)\n",
        "        logits = torch.mean(logits,dim=1)\n",
        "        pred = logits.argmax(1)\n",
        "        test_correct += (pred == labels).sum().item()\n",
        "\n",
        "test_acc = test_correct / num_test\n",
        "print(f'Test accuracy {test_acc:.4f}')\n",
        "\n",
        "# Plot the losses\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss_history, label=\"Train loss\")\n",
        "plt.plot(val_loss_history, label=\"Val loss\")\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_acc_history, label=\"Train acc\")\n",
        "plt.plot(val_acc_history, label=\"Val acc\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid"
      ],
      "metadata": {
        "id": "EJg7oBsGkZyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define our class"
      ],
      "metadata": {
        "id": "G9HqZ5WT0kDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Take the previous trained model, and fit a ML on top of it instead of the mlp\n",
        "class Hybrid(nn.Module):\n",
        "    def __init__(self, gnn, clf):\n",
        "        super(Hybrid, self).__init__()\n",
        "        self.gnn = gnn\n",
        "        self.clf = clf\n",
        "        self.train_features = None\n",
        "        self.train_labels = None\n",
        "        self.val_features = None\n",
        "        self.val_labels = None\n",
        "        self.test_features = None\n",
        "        self.test_labels = None\n",
        "\n",
        "    def cropped_foward_gnn(self, g, in_feat):\n",
        "        \"\"\"Same as the GNN forward function but without the mlp part\"\"\"\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        h = F.relu(h)\n",
        "\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            # Calculate graph representation by average readout.\n",
        "            hg = dgl.mean_nodes(g, 'h')\n",
        "    \n",
        "    def build_features_array(self, dataloader):\n",
        "        features = []\n",
        "        for batched_graph, labels in dataloader:\n",
        "            graphs = dgl.unbatch(batched_graph)\n",
        "            # Then, for each graph, we extract the node features and the label\n",
        "            for graph in graphs:\n",
        "                gnn_output = self.cropped_foward_gnn(batched_graph, batched_graph.ndata['x'].float())\n",
        "                gnn_output = torch.mean(gnn_output, dim=1)\n",
        "                gnn_output = torch.mean(gnn_output, dim=1)\n",
        "                features.append(gnn_output.detach().numpy())\n",
        "        return np.concatenate(features)\n",
        "\n",
        "    def build_features_and_labels_array(self, dataloader):\n",
        "        features = []\n",
        "        labels = []\n",
        "        for batched_graph, batched_label in tqdm(dataloader):\n",
        "            # First, unbatch the batched graph\n",
        "            graphs = dgl.unbatch(batched_graph)\n",
        "            # Then, for each graph, we extract the node features and the label\n",
        "            for graph, graph_label in zip(graphs, batched_label):\n",
        "                # Extract the node features\n",
        "                gnn_output = self.cropped_foward_gnn(graph, graph.ndata['x'].float())\n",
        "                gnn_output = torch.mean(gnn_output, dim=1)\n",
        "                gnn_output = torch.mean(gnn_output, dim=1)\n",
        "                features.append(gnn_output.detach().numpy())\n",
        "                # Extract the label\n",
        "                labels.append(graph_label.detach().numpy())\n",
        "        \n",
        "        return np.concatenate(features), np.array(labels)\n",
        "\n",
        "    def train_clf(self, dataloader):\n",
        "        features, labels = self.build_features_and_labels_array(dataloader)\n",
        "        self.clf.fit(features, labels)\n",
        "    \n",
        "    def predict_clf(self, dataloader):\n",
        "        features = self.build_features_array(dataloader)\n",
        "        return self.clf.predict(features)\n",
        "\n",
        "    def evaluate_clf(self, dataloader):\n",
        "        features, labels = self.build_features_and_labels_array(dataloader)\n",
        "        # Plot confusion matrix\n",
        "        cm = confusion_matrix(labels, self.clf.predict(features))\n",
        "        plt.figure(figsize=(10,10))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        return self.clf.score(features, labels)\n",
        "        \n"
      ],
      "metadata": {
        "id": "g8vORp0Fzk4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the approach on a Random Forest"
      ],
      "metadata": {
        "id": "aKvkYfuM0d0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
        "\n",
        "hybrid_model = Hybrid(model.to(\"cpu\"), clf)#.to(device)\n",
        "hybrid_model.train_clf(train_loader)\n",
        "hybrid_model.evaluate_clf(test_loader)"
      ],
      "metadata": {
        "id": "OQQRnfsYzzTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark three models"
      ],
      "metadata": {
        "id": "gifU3Tu00ig5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_benchmark = [RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0),\n",
        "                 SVC(gamma='auto'),\n",
        "                 XGBClassifier()]\n",
        "\n",
        "for clf in clf_benchmark:\n",
        "    hybrid_model = Hybrid(model.to(\"cpu\"), clf)\n",
        "    hybrid_model.train_clf(train_loader)\n",
        "    print(hybrid_model.evaluate_clf(test_loader))\n"
      ],
      "metadata": {
        "id": "NS05O_rP0UB7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}